{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the mnist data\n",
    "mnist = input_data.read_data_sets('MNIST_data/', one_hot=True)\n",
    "\n",
    "images = mnist.test.images\n",
    "labels = mnist.test.labels\n",
    "\n",
    "#print (mnist.test.images.shape)\n",
    "#print (mnist.test.labels.shape)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "W_1 = (np.random.randn(784,100))/tf.sqrt(784/2)\n",
    "b_1 = (np.random.randn(100,))/tf.sqrt(784/2)\n",
    "      \n",
    "W_2 = (np.random.randn(100,50))/tf.sqrt(100/2)\n",
    "b_2 = (np.random.randn(50,))/tf.sqrt(100/2)\n",
    "\n",
    "W = (np.random.randn(50,10))/tf.sqrt(50/2)\n",
    "b = (np.random.randn(10,))/tf.sqrt(50/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "        g = 1 /(1 + np.exp(-z))\n",
    "        return g\n",
    "\n",
    "def sigmoid_gradient(z): \n",
    "        g = (np.exp(-z))/(1 + np.exp(-z))**2 \n",
    "        return g\n",
    "\n",
    "def relu(z):\n",
    "        g = z * (z > 0)\n",
    "        return g\n",
    "\n",
    "def relu_gradient(z):\n",
    "        g = 1 * (z > 0)\n",
    "        return g     \n",
    "\n",
    "def softmax(z):\n",
    "        g = np.exp(z) / np.sum(np.exp(z), axis=0)\n",
    "        return g #np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " class Network(object):\n",
    "    def __init__(self, nnodes,activation_func):\n",
    "        self.sizes = nnodes\n",
    "        self.num_layers = len(nnodes)\n",
    "        self.activation_func = activation_func\n",
    "        self.biases = [np.random.randn(y,1) for y in nnodes[1:]]\n",
    "        self.weights = [np.random.randn(y,x)/np.sqrt(x/2)\n",
    "                        for x, y in zip(nnodes[:-1], nnodes[1:])]  #[784, 100, 50], [ 100, 50, 10 ]\n",
    "        self.costs=[]  # stores the costs during training\n",
    "        self.accuracies=[] # stores the accuracies during training \n",
    "        \n",
    "    def feedforward(self, inputs):\n",
    "        activations = []\n",
    "        activations.append(inputs) \n",
    "        zs= [] \n",
    "        \n",
    "        for l in range(len(self.weights)-1): #4-1 = 3\n",
    "            b = self.biases[l]\n",
    "            w = self.weights[l]\n",
    "            z = w @ inputs + b\n",
    "           \n",
    "            if self.activation_func[l]=='sigmoid':\n",
    "                activation = sigmoid(z)\n",
    "            elif self.activation_func[l]=='relu': \n",
    "                activation = relu(z)\n",
    "       \n",
    "            zs.append(z) \n",
    "            activations.append(activation)\n",
    "            inputs=activation  \n",
    "            \n",
    "        w = self.weights[-1]\n",
    "        b = self.biases[-1]\n",
    "        z = w @ inputs + b\n",
    "        activation = softmax(z)\n",
    "        activations.append(activation)\n",
    "        zs.append(z)\n",
    "        return activation,zs,activations\n",
    "    \n",
    "    def cost_function(self,minibatch):\n",
    "        # computes the cross entropy cost for a single minibatch\n",
    "        cost = 0;\n",
    "        for n in range(len(minibatch)):\n",
    "            x=minibatch[n][0]\n",
    "            y=minibatch[n][1]\n",
    "            prediction,dum1,dum2=self.feedforward(x)\n",
    "            cost += np.sum(-(y * np.log(prediction) - (1 - y) * np.log(1 - prediction)))\n",
    "\n",
    "        return cost \n",
    "\n",
    "    def Gradient_Descent(self, training_data, epoches,lr,batchsize,test_data=None):\n",
    "        # training_data is a list of tuples ([x1,x2],y) where y is the class and [x1,x2] is numpy array\n",
    "        # lr is the learning_rate, it is scalar\n",
    "        # epochs is a scalar value for the number of times the network is going to update its parameters        \n",
    "        \"\"\"        \n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        for j in range(epochs):\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+batchsize]\n",
    "                for k in range(0, n, batchsize)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_params(mini_batch,lr) \n",
    "                cost =self.cost_function(mini_batch)\n",
    "                self.costs.append(cost/len(mini_batch))\n",
    "            if test_data:\n",
    "                correct = self.evaluate(test_data)\n",
    "                print (\"Epoch {0}: {1} / {2} Cost: {3} Accuracy: {4}\".format(j, correct, n_test,cost/len(mini_batch),correct/n_test))\n",
    "                self.accuracies.append(correct/n_test)\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\".format(j))    \n",
    "        \"\"\"\n",
    "        if test_data: n_test = len(test_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            epoch_cost = 0\n",
    "            np.random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+batchsize]\n",
    "                for k in range(0, n, batchsize)]\n",
    "            #print(len(mini_batches))\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_params(mini_batch,lr) \n",
    "                cost =self.cost_function(mini_batch)\n",
    "                epoch_cost += (cost/len(mini_batch))\n",
    "            if test_data:\n",
    "                self.costs.append(epoch_cost/len(mini_batches))\n",
    "                correct = self.evaluate(test_data)\n",
    "                print (\"Epoch {0}: {1} / {2} Cost: {3} Accuracy: {4}\".format(j, correct, n_test, epoch_cost/len(mini_batches),correct/n_test))\n",
    "                self.accuracies.append(correct/n_test)\n",
    "            else:\n",
    "                print (\"Epoch {0} complete\".format(j))  \n",
    "        return self.costs  \n",
    "  \n",
    "        \n",
    "    def update_params(self, minibatch, lr):\n",
    "        # X is all the training data\n",
    "        # X contains pairs (x,y) where x is the feature vector and y its class label\n",
    "        N = len(minibatch) #batchsize\n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        #print(len(grad_b))\n",
    "        #print(grad_b[0].shape)\n",
    "        #Gradient Descent parameter updates\n",
    "        for x,y in minibatch:\n",
    "            delta_grad_b, delta_grad_w = self.backprop(x, y) \n",
    "            grad_b = [nb+dnb for nb, dnb in zip(grad_b, delta_grad_b)]\n",
    "            grad_w = [nw+dnw for nw, dnw in zip(grad_w, delta_grad_w)] \n",
    "    \n",
    "            self.weights = [w-lr*nw/N for w, nw in zip(self.weights, grad_w)]  \n",
    "            self.biases =  [b-lr*nb/N for b, nb in zip(self.biases,  grad_b)]\n",
    "\n",
    "        \n",
    "    def backprop(self, x, y):\n",
    "        #Returns (grad_b, grad_w) representing the\n",
    "        #gradients for the cost function  wrt b and w . \n",
    "        grad_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        grad_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        \n",
    "        dummy,zs,activations= self.feedforward(x) #dummy is not used below \n",
    "      \n",
    "        # Grad at the ouput layer \n",
    "        delta = activations[-1]- y\n",
    "\n",
    "        grad_b[-1] = delta\n",
    "        grad_w[-1] = np.dot(delta,activations[-2].T)\n",
    "\n",
    "        L = self.num_layers #  input layer + hidden layers + ouput layers = 4\n",
    "        \n",
    "        for l in range(1, L-1):\n",
    "            z = zs[-l-1]\n",
    "            if self.activation_func[-l]=='relu': \n",
    "                delta = np.dot(self.weights[-l].transpose(), delta) * relu_gradient(z)    \n",
    "            elif self.activation_func[-l]=='sigmoid':\n",
    "                delta = np.dot(self.weights[-l].transpose(), delta) * sigmoid_gradient(z)    \n",
    "            # gradients of the parameters of the l-th layer\n",
    "            grad_b[-l-1] = delta \n",
    "            grad_w[-l-1] = np.dot(delta, activations[-l-2].T) \n",
    "        \n",
    "        #print(grad_b[2].shape)\n",
    "        #print(len(grad_b))\n",
    "        return (grad_b, grad_w)\n",
    "    \n",
    "    def evaluate(self, X):\n",
    "        correct = 0\n",
    "        for n in range(len(X)):\n",
    "            x=X[n][0]\n",
    "            y=X[n][1]\n",
    "            prediction,dum1,dum2=self.feedforward(x)\n",
    "            correct += int(np.argmax(y) == np.argmax(prediction))\n",
    "        return correct\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nnodes=[784,100,50,10]\n",
    "learning_rate = 0.0001\n",
    "epochs = 20\n",
    "batch_size = 126\n",
    "#tr_batch= mnist.train.next_batch(batch_size)\n",
    "#training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "#training_results = [np.reshape(y,(10,1)) for y in tr_d[1]]\n",
    "x, y = mnist.train.images, mnist.train.labels\n",
    "x_t, y_t = mnist.test.images, mnist.test.labels\n",
    "training_inputs = [np.reshape(x, (784, 1)) for x in x]\n",
    "training_results = [np.reshape(y,(10,1)) for y in y]\n",
    "testing_inputs = [np.reshape(x, (784, 1)) for x in x_t]\n",
    "testing_results = [np.reshape(y,(10,1)) for y in y_t]\n",
    "train_data = list(zip(training_inputs, training_results))\n",
    "test_data = list(zip(testing_inputs, testing_results))\n",
    "activation_func=['sigmoid','relu'] # use 'sigmoid' or 'relu'\n",
    "nnet = Network(nnodes,activation_func)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 6153 / 10000 Cost: 1.275790707043728 Accuracy: 0.6153\n",
      "Epoch 1: 7166 / 10000 Cost: 0.984687961575514 Accuracy: 0.7166\n",
      "Epoch 2: 7788 / 10000 Cost: 0.69456366226581 Accuracy: 0.7788\n",
      "Epoch 3: 7993 / 10000 Cost: 0.4562528868666425 Accuracy: 0.7993\n",
      "Epoch 4: 8186 / 10000 Cost: 0.32368508768098536 Accuracy: 0.8186\n",
      "Epoch 5: 8374 / 10000 Cost: 0.2549802405450448 Accuracy: 0.8374\n",
      "Epoch 6: 8490 / 10000 Cost: 0.21577570133182533 Accuracy: 0.849\n",
      "Epoch 7: 8587 / 10000 Cost: 0.19093756731253378 Accuracy: 0.8587\n",
      "Epoch 8: 8677 / 10000 Cost: 0.17349604397948956 Accuracy: 0.8677\n",
      "Epoch 9: 8743 / 10000 Cost: 0.1610764192995576 Accuracy: 0.8743\n",
      "Epoch 10: 8802 / 10000 Cost: 0.1513394453178272 Accuracy: 0.8802\n",
      "Epoch 11: 8842 / 10000 Cost: 0.1436559648842627 Accuracy: 0.8842\n",
      "Epoch 12: 8889 / 10000 Cost: 0.1374826793122531 Accuracy: 0.8889\n",
      "Epoch 13: 8906 / 10000 Cost: 0.13257590920189874 Accuracy: 0.8906\n",
      "Epoch 14: 8929 / 10000 Cost: 0.12832359927341547 Accuracy: 0.8929\n",
      "Epoch 15: 8958 / 10000 Cost: 0.12467246669990051 Accuracy: 0.8958\n",
      "Epoch 16: 8974 / 10000 Cost: 0.12163305550534648 Accuracy: 0.8974\n",
      "Epoch 17: 8979 / 10000 Cost: 0.119034492838103 Accuracy: 0.8979\n",
      "Epoch 18: 9003 / 10000 Cost: 0.11686085751942654 Accuracy: 0.9003\n",
      "Epoch 19: 9007 / 10000 Cost: 0.11475404517403481 Accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.275790707043728,\n",
       " 0.98468796157551397,\n",
       " 0.69456366226581001,\n",
       " 0.4562528868666425,\n",
       " 0.32368508768098536,\n",
       " 0.25498024054504481,\n",
       " 0.21577570133182533,\n",
       " 0.19093756731253378,\n",
       " 0.17349604397948956,\n",
       " 0.1610764192995576,\n",
       " 0.1513394453178272,\n",
       " 0.14365596488426269,\n",
       " 0.13748267931225311,\n",
       " 0.13257590920189874,\n",
       " 0.12832359927341547,\n",
       " 0.12467246669990051,\n",
       " 0.12163305550534648,\n",
       " 0.119034492838103,\n",
       " 0.11686085751942654,\n",
       " 0.11475404517403481]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnet.Gradient_Descent(train_data,epochs,learning_rate,batch_size,test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = [1.275790707043728,\n",
    " 0.98468796157551397,\n",
    " 0.69456366226581001,\n",
    " 0.4562528868666425,\n",
    " 0.32368508768098536,\n",
    " 0.25498024054504481,\n",
    " 0.21577570133182533,\n",
    " 0.19093756731253378,\n",
    " 0.17349604397948956,\n",
    " 0.1610764192995576,\n",
    " 0.1513394453178272,\n",
    " 0.14365596488426269,\n",
    " 0.13748267931225311,\n",
    " 0.13257590920189874,\n",
    " 0.12832359927341547,\n",
    " 0.12467246669990051,\n",
    " 0.12163305550534648,\n",
    " 0.119034492838103,\n",
    " 0.11686085751942654,\n",
    " 0.11475404517403481]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMASF28Cqy/UAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1224a3b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.plot(cost)\n",
    "plt.xticks(np.arange(0,22,2))\n",
    "plt.title('model loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
